{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2749070,"sourceType":"datasetVersion","datasetId":1619941}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/amazon-stock-price-all-time/Amazon.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.plot(data['Date'], data['Close'])\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(['Date', 'Adj Close'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window = 3\nfeatures = 5\ndef prepare_data(data):\n    prepared = []\n    for row in range(len(data)//window):\n        new_row = [0] * (window) * features + [0]\n        for f in range(features):\n            for t in range(window):\n                new_row[window*f + t] = data.iloc[row + t, f]\n        new_row[-1] = data['Close'].iloc[row + window]\n        prepared.append(new_row)\n    prepared = pd.DataFrame(prepared)\n    prepared.columns = [f'Open_t-{window-t}' for t in range(window)] + [f'High_t-{window-t}' for t in range(window)] + [f'Low_t-{window-t}' for t in range(window)] + [f'Close_t-{window-t}' for t in range(window)] + [f'Volume_t-{window-t}' for t in range(window)] + [\"Close\"]\n    columns = []\n    for t in range(window):\n        columns += [f'Open_t-{window-t}', f'High_t-{window-t}', f'Low_t-{window-t}', f'Close_t-{window-t}', f'Volume_t-{window-t}']\n    return prepared, columns\ndata_train, columns = prepare_data(data)\n#data_train = data_train[[f'Close_t-{window-t}' for t in range(window)] + ['Close']]\ntarget = data_train['Close']\nprint(data_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndata_train = scaler.fit_transform(data_train.drop('Close', axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = pd.DataFrame(data_train, columns=columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train['Close'] = target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_pass = []\nfor t in range(window):\n    columns_to_pass += [f'Close_t-{window-t}']\n    #columns_to_pass += [f'Volume_t-{window-t}']\ncolumns_to_pass += ['Close']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = data_train[columns_to_pass]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = 8 * len(data_train)//10\nX_train = data_train[:split].drop('Close', axis=1)\ny_train = data_train[:split]['Close']\n\nX_test = data_train[split:].drop('Close', axis=1)\ny_test = data_train[split:]['Close']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(np.array(X_train)).float().unsqueeze(-1)\ny_train = torch.tensor(np.array(y_train)).float().unsqueeze(-1)\n\nX_test = torch.tensor(np.array(X_test)).float().unsqueeze(-1)\ny_test = torch.tensor(np.array(y_test)).float().unsqueeze(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n    \ntrain_dataset = TimeSeriesDataset(X_train, y_train)\ntest_dataset = TimeSeriesDataset(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batchsize = 16\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batchsize,shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batchsize,shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _, batch in enumerate(train_loader):\n    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n    print(x_batch.shape, y_batch.shape)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_stacked_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_stacked_layers = num_stacked_layers\n\n        self.lstm = nn.RNN(input_size, hidden_size, num_stacked_layers, \n                            batch_first=True)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        \n        out, _ = self.lstm(x, h0)\n        print(out)\n        print(_)\n        out = self.relu(out)\n        out = self.fc(out[:, -1, :])\n        print(out)\n        return out\n\nmodel = LSTM(1, 5, 1)\nmodel.to(device)\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch():\n    model.train(True)\n    print(f'Epoch: {epoch + 1}')\n    running_loss = 0.0\n    \n    for batch_index, batch in enumerate(train_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        output = model(x_batch)\n        loss = loss_function(output, y_batch)\n        running_loss += loss.item()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_index % 50 == 0:  # print every 100 batches\n            avg_loss_across_batches = running_loss / 50\n            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n                                                    avg_loss_across_batches))\n            running_loss = 0.0\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate_one_epoch():\n    model.train(False)\n    running_loss = 0.0\n    \n    for batch_index, batch in enumerate(test_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        \n        with torch.inference_mode():\n            output = model(x_batch)\n            loss = loss_function(output, y_batch)\n            running_loss += loss.item()\n\n    avg_loss_across_batches = running_loss / len(test_loader)\n    \n    print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n    print('***************************************************')\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 100\nfor epoch in range(NUM_EPOCHS):\n    train_one_epoch()\n    validate_one_epoch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    predicted = model(X_train.to(device)).to('cpu').numpy()\n\nplt.plot(y_train, label='Actual Close')\nplt.plot(predicted, label='Predicted Close')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    predicted = model(X_test.to(device)).to('cpu').numpy()\n\nplt.plot(y_test, label='Actual Close')\nplt.plot(predicted, label='Predicted Close')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}